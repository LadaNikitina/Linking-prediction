{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726baddc",
   "metadata": {},
   "source": [
    "# Dialogue Graph Auto Construction based on data with a regular structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22fcd1",
   "metadata": {},
   "source": [
    "Goal: Extract regular structures from the data by building a dialogue graph\n",
    "    \n",
    "Tasks: \n",
    "* Cluster dialog data using embeddings of pre-trained models (BERT, ConveRT, S-BERT…)\n",
    "* Evaluate the quality of clustering using intent’s labeling of Multi-WoZ dataset \n",
    "* Linking clusters of dialogs using naive approaches (Estimation of Probabilities by Frequency Models)\n",
    "* Try other approaches (Deep Neural Networks) for linking clusters and improve the naive approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72eb4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 17:48:39.350488: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 17:48:40.544845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 17:48:40.544961: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 17:48:40.544972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_add\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3868698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b1c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/cephfs/home/ledneva/final_work/common_utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3546905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_function import get_data\n",
    "from functions_GTN import preprocessing\n",
    "from early_stopping_tools import LRScheduler, EarlyStopping\n",
    "from preprocess import Clusters, get_accuracy_k, get_all_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf6d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_fastgtn import FastGTNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fdc9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_num_clusters = 400\n",
    "second_num_clusters = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc864bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (/home/ledneva/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91090b7ed64578aea11a265113fda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embeddings are loading...\n",
      "The first stage of clustering has begun...\n",
      "The second stage of clustering has begun...\n",
      "The searching clusters for test and validation has begun...\n",
      "Intent metric conveRT-one-prev, 60 two-stage clusters,                 user: 0.7370199324793312\n",
      "Intent metric conveRT-one-prev, 60 two-stage clusters,                 system: 0.7554430398668559\n"
     ]
    }
   ],
   "source": [
    "path = \"/cephfs/home/ledneva/final_work/convert_one_prev_embeddings.csv\"\n",
    "clusters = Clusters(first_num_clusters, second_num_clusters, path)\n",
    "clusters.form_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242efc4b",
   "metadata": {},
   "source": [
    "## 4.1 GTN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe0e1d",
   "metadata": {},
   "source": [
    "Functions generating butches for two types of graphs and metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7a2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39289fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "batch_size = 512\n",
    "embs_dim = len(clusters.user_cluster_embs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f407a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cluster_emb = np.zeros(embs_dim)\n",
    "fake_cluster_emb = np.zeros(embs_dim)\n",
    "\n",
    "embs = np.concatenate([clusters.user_cluster_embs, clusters.system_cluster_embs, [null_cluster_emb, fake_cluster_emb]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a61a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train_x, user_train_y, sys_train_x, sys_train_y = get_data(clusters.train_dataset, top_k, second_num_clusters, clusters.train_user_df, clusters.train_system_df)\n",
    "user_test_x, user_test_y, sys_test_x, sys_test_y = get_data(clusters.test_dataset, top_k, second_num_clusters, clusters.test_user_df, clusters.test_system_df)\n",
    "user_valid_x, user_valid_y, sys_valid_x, sys_valid_y = get_data(clusters.validation_dataset, top_k, second_num_clusters, clusters.valid_user_df, clusters.valid_system_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09306168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 111/111 [03:20<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "user_train_matrices, user_train_node_embs, user_train_labels = preprocessing(user_train_x, \n",
    "                                                                             user_train_y, \n",
    "                                                                             batch_size,\n",
    "                                                                             top_k, embs,\n",
    "                                                                             second_num_clusters, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5ee3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 111/111 [03:17<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "sys_train_matrices, sys_train_node_embs, sys_train_labels = preprocessing(sys_train_x, \n",
    "                                                                          sys_train_y, \n",
    "                                                                          batch_size,\n",
    "                                                                          top_k, embs,\n",
    "                                                                          second_num_clusters, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a3099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [00:26<00:00,  1.77s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:25<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "user_test_matrices, user_test_node_embs, user_test_labels = preprocessing(user_test_x, \n",
    "                                                                          user_test_y, \n",
    "                                                                          batch_size,\n",
    "                                                                          top_k, embs,\n",
    "                                                                          second_num_clusters, 0)\n",
    "sys_test_matrices, sys_test_node_embs, sys_test_labels = preprocessing(sys_test_x,\n",
    "                                                                       sys_test_y, \n",
    "                                                                       batch_size,\n",
    "                                                                       top_k, embs,\n",
    "                                                                       second_num_clusters, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95c0d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [00:27<00:00,  1.81s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:26<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "user_valid_matrices, user_valid_node_embs, user_valid_labels = preprocessing(user_valid_x, \n",
    "                                                                            user_valid_y, \n",
    "                                                                            batch_size,\n",
    "                                                                            top_k, embs,\n",
    "                                                                            second_num_clusters, 1)\n",
    "sys_valid_matrices, sys_valid_node_embs, sys_valid_labels = preprocessing(sys_valid_x,\n",
    "                                                                         sys_valid_y, \n",
    "                                                                         batch_size,\n",
    "                                                                         top_k, embs,\n",
    "                                                                         second_num_clusters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf9be7",
   "metadata": {},
   "source": [
    "## User model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f06bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_GTN_arguments():\n",
    "    epoch = 30\n",
    "    model = 'FastGTN'\n",
    "    node_dim = 512\n",
    "    num_channels = 5\n",
    "    lr = 0.0005\n",
    "    weight_decay = 0.0005\n",
    "    num_layers = 3\n",
    "    channel_agg = 'mean'\n",
    "    remove_self_loops = False\n",
    "    beta = 1\n",
    "    non_local = False\n",
    "    non_local_weight = 0\n",
    "    num_FastGTN_layers = 2\n",
    "    top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "363d2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_args = user_GTN_arguments()\n",
    "user_args.num_nodes = user_train_node_embs[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "772c2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = FastGTNs(num_edge_type = 4,\n",
    "                w_in = user_train_node_embs[0].shape[1],\n",
    "                num_class=second_num_clusters,\n",
    "                num_nodes = user_train_node_embs[0].shape[0],\n",
    "                args = user_args)\n",
    "\n",
    "user_model.to(device)\n",
    "user_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1134aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "user_optimizer = torch.optim.Adam(user_model.parameters(), lr = user_args.lr)\n",
    "user_lr_scheduler = LRScheduler(user_optimizer)\n",
    "user_early_stopping = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51cd8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 3.5486, valid loss 3.0496\n",
      "Epoch 1, train loss 2.6952, valid loss 2.5077\n",
      "Epoch 2, train loss 2.4510, valid loss 2.4064\n",
      "Epoch 3, train loss 2.3867, valid loss 2.3713\n",
      "Epoch 4, train loss 2.3542, valid loss 2.3506\n",
      "Epoch 5, train loss 2.3312, valid loss 2.3346\n",
      "Epoch 6, train loss 2.3123, valid loss 2.3202\n",
      "Epoch 7, train loss 2.2955, valid loss 2.3060\n",
      "Epoch 8, train loss 2.2797, valid loss 2.2917\n",
      "Epoch 9, train loss 2.2647, valid loss 2.2780\n",
      "Epoch 10, train loss 2.2505, valid loss 2.2653\n",
      "Epoch 11, train loss 2.2375, valid loss 2.2541\n",
      "Epoch 12, train loss 2.2256, valid loss 2.2442\n",
      "Epoch 13, train loss 2.2146, valid loss 2.2352\n",
      "Epoch 14, train loss 2.2044, valid loss 2.2270\n",
      "Epoch 15, train loss 2.1947, valid loss 2.2196\n",
      "Epoch 16, train loss 2.1857, valid loss 2.2124\n",
      "Epoch 17, train loss 2.1771, valid loss 2.2057\n",
      "Epoch 18, train loss 2.1690, valid loss 2.1999\n",
      "Epoch 19, train loss 2.1613, valid loss 2.1939\n",
      "Epoch 20, train loss 2.1540, valid loss 2.1881\n",
      "Epoch 21, train loss 2.1470, valid loss 2.1829\n",
      "Epoch 22, train loss 2.1403, valid loss 2.1779\n",
      "Epoch 23, train loss 2.1339, valid loss 2.1731\n",
      "Epoch 24, train loss 2.1279, valid loss 2.1688\n",
      "Epoch 25, train loss 2.1221, valid loss 2.1644\n",
      "Epoch 26, train loss 2.1165, valid loss 2.1604\n",
      "Epoch 27, train loss 2.1113, valid loss 2.1566\n",
      "Epoch 28, train loss 2.1063, valid loss 2.1532\n",
      "Epoch 29, train loss 2.1014, valid loss 2.1497\n"
     ]
    }
   ],
   "source": [
    "train_num_batches = len(user_train_matrices)\n",
    "valid_num_batches = len(user_valid_matrices)\n",
    "old_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(user_args.epoch):\n",
    "    train_epoch_loss = 0\n",
    "    \n",
    "    for num_iter in range(train_num_batches):\n",
    "        A = user_train_matrices[num_iter]\n",
    "        node_features = user_train_node_embs[num_iter]\n",
    "        y_true = torch.from_numpy(user_train_labels[num_iter]).to(device)\n",
    "        \n",
    "        user_model.zero_grad()\n",
    "        user_model.train()\n",
    "\n",
    "        y_train = user_model(A, node_features, epoch=epoch)\n",
    "        if -1 in y_true:\n",
    "            train_loss = user_loss(y_train[y_true != -1], y_true[y_true != -1])\n",
    "        else:\n",
    "            train_loss = user_loss(y_train, y_true)\n",
    "        # тут считать лосс, выкинуть фейки\n",
    "\n",
    "        train_loss.backward()\n",
    "        user_optimizer.step()\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        \n",
    "    train_epoch_loss /= train_num_batches\n",
    "\n",
    "    valid_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for num_iter in range(valid_num_batches):\n",
    "            A = user_valid_matrices[num_iter]\n",
    "            node_features = user_valid_node_embs[num_iter]\n",
    "            y_true = torch.from_numpy(user_valid_labels[num_iter]).to(device)\n",
    "            \n",
    "            y_valid = user_model.forward(A, node_features, epoch=epoch)\n",
    "            if -1 in y_true:\n",
    "                valid_loss = user_loss(y_valid[y_true != -1], y_true[y_true != -1])\n",
    "            else:\n",
    "                valid_loss = user_loss(y_valid, y_true)\n",
    "\n",
    "            # тут считать лосс, выкинуть фейки\n",
    "            valid_epoch_loss += valid_loss.detach().item()\n",
    "        \n",
    "        valid_epoch_loss /= valid_num_batches\n",
    "    \n",
    "        if abs(valid_epoch_loss - old_valid_loss) < 1e-4 or old_valid_loss < valid_epoch_loss:\n",
    "            break\n",
    "        old_valid_loss = valid_epoch_loss\n",
    "    \n",
    "    print(f'Epoch {epoch}, train loss {train_epoch_loss:.4f}, valid loss {valid_epoch_loss:.4f}')  \n",
    "    \n",
    "    user_lr_scheduler(valid_epoch_loss)\n",
    "    user_early_stopping(valid_epoch_loss)\n",
    "    \n",
    "    if user_early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df25c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model.eval()\n",
    "test_num_batches = len(user_test_matrices)\n",
    "user_true = []\n",
    "user_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for num_iter in range(test_num_batches):\n",
    "        A = user_test_matrices[num_iter]\n",
    "        node_features = user_test_node_embs[num_iter]\n",
    "        y_true = torch.from_numpy(user_test_labels[num_iter])\n",
    "        y_test = torch.softmax(user_model.forward(A, node_features), 1)\n",
    "        \n",
    "        if -1 in y_true:\n",
    "            user_test += y_test[y_true != -1].tolist()\n",
    "            user_true += y_true[y_true != -1].tolist()\n",
    "        else:\n",
    "            user_test += y_test.tolist()\n",
    "            user_true += y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf29bbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER metric\n",
      "Acc@1: 0.39411810411810416\n",
      "Acc@3: 0.6570222166722167\n",
      "Acc@5: 0.7709083694083694\n",
      "Acc@10: 0.9035682484182485\n"
     ]
    }
   ],
   "source": [
    "print(\"USER metric\")\n",
    "\n",
    "print(\"Acc@1:\", get_accuracy_k(1, clusters.test_user_df, user_test, clusters.test_dataset, 0))\n",
    "print(\"Acc@3:\", get_accuracy_k(3, clusters.test_user_df, user_test, clusters.test_dataset, 0))\n",
    "print(\"Acc@5:\", get_accuracy_k(5, clusters.test_user_df, user_test, clusters.test_dataset, 0))\n",
    "print(\"Acc@10:\", get_accuracy_k(10, clusters.test_user_df, user_test, clusters.test_dataset, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5e3c9",
   "metadata": {},
   "source": [
    "## System model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a685381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sys_GTN_arguments():\n",
    "    epoch = 30\n",
    "    model = 'FastGTN'\n",
    "    node_dim = 512\n",
    "    num_channels = 5\n",
    "    lr = 0.0005\n",
    "    weight_decay = 0.0005\n",
    "    num_layers = 3\n",
    "    channel_agg = 'mean'\n",
    "    remove_self_loops = False\n",
    "    beta = 1\n",
    "    non_local = False\n",
    "    non_local_weight = 0\n",
    "    num_FastGTN_layers = 2\n",
    "    top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "452d7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_args = sys_GTN_arguments()\n",
    "sys_args.num_nodes = sys_train_node_embs[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7587ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_model = FastGTNs(num_edge_type = 4,\n",
    "                w_in = sys_train_node_embs[0].shape[1],\n",
    "                num_class=second_num_clusters, # разобраться что с фейками\n",
    "                num_nodes = sys_train_node_embs[0].shape[0],\n",
    "                args = sys_args)\n",
    "\n",
    "sys_optimizer = torch.optim.Adam(sys_model.parameters(), lr=sys_args.lr)\n",
    "sys_lr_scheduler = LRScheduler(sys_optimizer)\n",
    "sys_early_stopping = EarlyStopping()\n",
    "\n",
    "sys_model.cuda()\n",
    "sys_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5beeba72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 3.4166, valid loss 2.8127\n",
      "Epoch 1, train loss 2.3104, valid loss 2.1885\n",
      "Epoch 2, train loss 2.0958, valid loss 2.1332\n",
      "Epoch 3, train loss 2.0651, valid loss 2.1182\n",
      "Epoch 4, train loss 2.0506, valid loss 2.1131\n",
      "Epoch 5, train loss 2.0414, valid loss 2.1103\n",
      "Epoch 6, train loss 2.0344, valid loss 2.1083\n",
      "Epoch 7, train loss 2.0285, valid loss 2.1062\n",
      "Epoch 8, train loss 2.0232, valid loss 2.1045\n",
      "Epoch 9, train loss 2.0184, valid loss 2.1025\n",
      "Epoch 10, train loss 2.0137, valid loss 2.1007\n",
      "Epoch 11, train loss 2.0091, valid loss 2.0984\n",
      "Epoch 12, train loss 2.0044, valid loss 2.0961\n",
      "Epoch 13, train loss 1.9995, valid loss 2.0934\n",
      "Epoch 14, train loss 1.9944, valid loss 2.0906\n",
      "Epoch 15, train loss 1.9892, valid loss 2.0876\n",
      "Epoch 16, train loss 1.9838, valid loss 2.0846\n",
      "Epoch 17, train loss 1.9784, valid loss 2.0813\n",
      "Epoch 18, train loss 1.9730, valid loss 2.0779\n",
      "Epoch 19, train loss 1.9677, valid loss 2.0746\n",
      "Epoch 20, train loss 1.9624, valid loss 2.0713\n",
      "Epoch 21, train loss 1.9575, valid loss 2.0681\n",
      "Epoch 22, train loss 1.9527, valid loss 2.0651\n",
      "Epoch 23, train loss 1.9482, valid loss 2.0624\n",
      "Epoch 24, train loss 1.9441, valid loss 2.0602\n",
      "Epoch 25, train loss 1.9403, valid loss 2.0581\n",
      "Epoch 26, train loss 1.9368, valid loss 2.0564\n",
      "Epoch 27, train loss 1.9336, valid loss 2.0551\n",
      "Epoch 28, train loss 1.9307, valid loss 2.0538\n",
      "Epoch 29, train loss 1.9279, valid loss 2.0527\n"
     ]
    }
   ],
   "source": [
    "train_num_batches = len(sys_train_matrices)\n",
    "valid_num_batches = len(sys_valid_matrices)\n",
    "old_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(sys_args.epoch):\n",
    "    train_epoch_loss = 0\n",
    "    \n",
    "    for num_iter in range(train_num_batches):\n",
    "        A = sys_train_matrices[num_iter]\n",
    "        node_features = sys_train_node_embs[num_iter]\n",
    "        y_true = torch.from_numpy(sys_train_labels[num_iter]).to(device)\n",
    "        \n",
    "        sys_model.zero_grad()\n",
    "        sys_model.train()\n",
    "\n",
    "        y_train = sys_model(A, node_features, epoch=epoch)\n",
    "        if -1 in y_true:\n",
    "            train_loss = sys_loss(y_train[y_true != -1], y_true[y_true != -1])\n",
    "        else:\n",
    "            train_loss = sys_loss(y_train, y_true)\n",
    "        # тут считать лосс, выкинуть фейки\n",
    "\n",
    "        train_loss.backward()\n",
    "        sys_optimizer.step()\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        \n",
    "    train_epoch_loss /= train_num_batches\n",
    "\n",
    "    valid_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for num_iter in range(valid_num_batches):\n",
    "            A = sys_valid_matrices[num_iter]\n",
    "            node_features = sys_valid_node_embs[num_iter]\n",
    "            y_true = torch.from_numpy(sys_valid_labels[num_iter]).to(device)\n",
    "            \n",
    "            y_valid = sys_model.forward(A, node_features, epoch=epoch)\n",
    "            if -1 in y_true:\n",
    "                valid_loss = sys_loss(y_valid[y_true != -1], y_true[y_true != -1])\n",
    "            else:\n",
    "                valid_loss = sys_loss(y_valid, y_true)\n",
    "\n",
    "            # тут считать лосс, выкинуть фейки\n",
    "            valid_epoch_loss += valid_loss.detach().item()\n",
    "        \n",
    "        valid_epoch_loss /= valid_num_batches\n",
    "        \n",
    "        if abs(valid_epoch_loss - old_valid_loss) < 1e-4 or old_valid_loss < valid_epoch_loss:\n",
    "            break\n",
    "        old_valid_loss = valid_epoch_loss\n",
    "    \n",
    "    print(f'Epoch {epoch}, train loss {train_epoch_loss:.4f}, valid loss {valid_epoch_loss:.4f}')  \n",
    "    \n",
    "    sys_lr_scheduler(valid_epoch_loss)\n",
    "    sys_early_stopping(valid_epoch_loss)\n",
    "    \n",
    "    if sys_early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7a026c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_model.eval()\n",
    "test_num_batches = len(sys_test_matrices)\n",
    "sys_true = []\n",
    "sys_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for num_iter in range(test_num_batches):\n",
    "        A = sys_test_matrices[num_iter]\n",
    "        node_features = sys_test_node_embs[num_iter]\n",
    "        y_true = torch.from_numpy(sys_test_labels[num_iter])\n",
    "        y_test = torch.softmax(sys_model.forward(A, node_features), 1)\n",
    "        \n",
    "        if -1 in y_true:\n",
    "            sys_test += y_test[y_true != -1].tolist()\n",
    "            sys_true += y_true[y_true != -1].tolist()\n",
    "        else:\n",
    "            sys_test += y_test.tolist()\n",
    "            sys_true += y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "675609d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM metric\n",
      "Acc@1: 0.3633423909423909\n",
      "Acc@3: 0.7104068958818959\n",
      "Acc@5: 0.8322004134754134\n",
      "Acc@10: 0.9161302558552559\n"
     ]
    }
   ],
   "source": [
    "print(\"SYSTEM metric\")\n",
    "\n",
    "print(\"Acc@1:\", get_accuracy_k(1, clusters.test_system_df, sys_test, clusters.test_dataset, 1))\n",
    "print(\"Acc@3:\", get_accuracy_k(3, clusters.test_system_df, sys_test, clusters.test_dataset, 1))\n",
    "print(\"Acc@5:\", get_accuracy_k(5, clusters.test_system_df, sys_test, clusters.test_dataset, 1))\n",
    "print(\"Acc@10:\", get_accuracy_k(10, clusters.test_system_df, sys_test, clusters.test_dataset, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "386c77c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL metric\n",
      "Acc@1: 0.37873024753024753\n",
      "Acc@3: 0.6837145562770564\n",
      "Acc@5: 0.8015543914418916\n",
      "Acc@10: 0.9098492521367522\n"
     ]
    }
   ],
   "source": [
    "print(\"ALL metric\")\n",
    "print(\"Acc@1:\", get_all_accuracy_k(1, clusters.test_user_df, clusters.test_system_df, user_test, sys_test, clusters.test_dataset))\n",
    "print(\"Acc@3:\", get_all_accuracy_k(3, clusters.test_user_df, clusters.test_system_df, user_test, sys_test, clusters.test_dataset))\n",
    "print(\"Acc@5:\", get_all_accuracy_k(5, clusters.test_user_df, clusters.test_system_df, user_test, sys_test, clusters.test_dataset))\n",
    "print(\"Acc@10:\", get_all_accuracy_k(10, clusters.test_user_df, clusters.test_system_df, user_test, sys_test, clusters.test_dataset))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
