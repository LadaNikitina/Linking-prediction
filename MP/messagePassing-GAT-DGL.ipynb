{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726baddc",
   "metadata": {},
   "source": [
    "# Dialogue Graph Auto Construction based on data with a regular structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22fcd1",
   "metadata": {},
   "source": [
    "Goal: Extract regular structures from the data by building a dialogue graph\n",
    "    \n",
    "Tasks: \n",
    "* Cluster dialog data using embeddings of pre-trained models (BERT, ConveRT, S-BERT…)\n",
    "* Evaluate the quality of clustering using intent’s labeling of Multi-WoZ dataset \n",
    "* Linking clusters of dialogs using naive approaches (Estimation of Probabilities by Frequency Models)\n",
    "* Try other approaches (Deep Neural Networks) for linking clusters and improve the naive approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "\n",
    "from preprocess import Clusters, get_accuracy_k, get_all_accuracy_k, get_all_accuracy_printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ead781",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f64c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"MP-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/cephfs/home/ledneva/final_work/common_utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_function_uttr_embs import get_data\n",
    "from GAT_functions_uttr_embs import get_data_dgl_no_cycles\n",
    "from early_stopping_tools import LRScheduler, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_num_clusters = 400\n",
    "second_num_clusters = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33449ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_path = \"/cephfs/home/ledneva/final_work/convert_one_prev_embeddings.csv\"\n",
    "clusters = Clusters(first_num_clusters, second_num_clusters, embs_path)\n",
    "clusters.form_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32905eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting unique intents\n",
    "unique_intent = []\n",
    "\n",
    "for intents in clusters.user_df['intent']:\n",
    "    unique_intent += intents\n",
    "for intents in clusters.system_df['intent']:\n",
    "    unique_intent += intents\n",
    "\n",
    "unique_intent = list(set(unique_intent))  \n",
    "num_intents = len(unique_intent)\n",
    "all_intents = []\n",
    "\n",
    "for i in range(second_num_clusters):\n",
    "    cluster = clusters.train_user_df[clusters.train_user_df['cluster'] == i]\n",
    "\n",
    "    intents = []\n",
    "    for intent_arr in cluster['intent']:\n",
    "        intents += intent_arr\n",
    "\n",
    "    intent_count = np.zeros(num_intents)\n",
    "    for j, intent in enumerate(unique_intent):\n",
    "        intent_count[j] = intents.count(intent)\n",
    "    all_intents.append(np.array(intent_count) / sum(intent_count))\n",
    "\n",
    "for i in range(second_num_clusters):\n",
    "    cluster = clusters.train_system_df[clusters.train_system_df['cluster'] == i]\n",
    "\n",
    "    intents = []\n",
    "    for intent_arr in cluster['intent']:\n",
    "        intents += intent_arr\n",
    "\n",
    "    intent_count = np.zeros(num_intents)\n",
    "    for j, intent in enumerate(unique_intent):\n",
    "        intent_count[j] = intents.count(intent)\n",
    "    all_intents.append(np.array(intent_count) / sum(intent_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca206193",
   "metadata": {},
   "source": [
    "## 4.3 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe0e1d",
   "metadata": {},
   "source": [
    "Functions generating butches for two types of graphs and metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc95d7",
   "metadata": {},
   "source": [
    "## 4.4 Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8091c",
   "metadata": {},
   "source": [
    "Data generation and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a61a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train_x, user_train_y, sys_train_x, sys_train_y = get_data(clusters.train_dataset, \n",
    "                                                                top_k, second_num_clusters, \n",
    "                                                                clusters.train_user_df, \n",
    "                                                                clusters.train_system_df,\n",
    "                                                                np.array(clusters.train_user_embs),\n",
    "                                                                np.array(clusters.train_system_embs))\n",
    "user_test_x, user_test_y, sys_test_x, sys_test_y = get_data(clusters.test_dataset, \n",
    "                                                            top_k, second_num_clusters,\n",
    "                                                            clusters.test_user_df,\n",
    "                                                            clusters.test_system_df,\n",
    "                                                            np.array(clusters.test_user_embs),\n",
    "                                                            np.array(clusters.test_system_embs))\n",
    "user_valid_x, user_valid_y, sys_valid_x, sys_valid_y = get_data(clusters.validation_dataset, \n",
    "                                                                top_k, second_num_clusters,\n",
    "                                                                clusters.valid_user_df, \n",
    "                                                                clusters.valid_system_df,\n",
    "                                                                np.array(clusters.valid_user_embs),\n",
    "                                                                np.array(clusters.valid_system_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09306168",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train_data = get_data_dgl_no_cycles(user_train_x, user_train_y, 1, top_k, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ee3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_train_data = get_data_dgl_no_cycles(sys_train_x, sys_train_y, 1, top_k, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_data = get_data_dgl_no_cycles(user_test_x, user_test_y, 0, top_k, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_test_data = get_data_dgl_no_cycles(sys_test_x, sys_test_y, 0, top_k, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_valid_data = get_data_dgl_no_cycles(user_valid_x, user_valid_y, 1, top_k, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_valid_data = get_data_dgl_no_cycles(sys_valid_x, sys_valid_y, 1, top_k, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаемые веса для суммирования\n",
    "linear_weights = np.zeros(top_k)\n",
    "linear_weights[...] = 1 / top_k\n",
    "linear_weights = torch.tensor(linear_weights).view(1, -1)\n",
    "linear_weights = linear_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5adef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_embs_dim = len(all_intents[0])\n",
    "centre_embs_dim = len(clusters.user_cluster_embs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb34616",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comps = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_embs_dim = num_comps\n",
    "learn_emb = nn.Parameter(\n",
    "            torch.Tensor(2 * second_num_clusters + 1, learn_embs_dim), requires_grad=False\n",
    ")\n",
    "learn_emb = torch.Tensor(nn.init.xavier_uniform_(learn_emb))\n",
    "# обучаемый эмбеддинг\n",
    "# weights = torch.Tensor(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cluster_centre_emb = np.zeros(centre_embs_dim)\n",
    "null_cluster_intent_emb = np.zeros(intent_embs_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "centre_mass = torch.Tensor(np.concatenate([clusters.user_cluster_embs, \n",
    "                                           clusters.system_cluster_embs, \n",
    "                                           [null_cluster_centre_emb]])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_embs = torch.Tensor(np.concatenate([all_intents, [null_cluster_intent_emb]])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24348632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_cluster_intents, system_cluster_intents - intents\n",
    "# clusters.user_cluster_embs, clusters.system_cluster_embs - center of mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0ad83",
   "metadata": {},
   "source": [
    "## 4.5 Prediction of user clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d56a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 2048\n",
    "embs_dim = 512 + len(centre_mass[0]) + learn_embs_dim + len(intent_embs[0])\n",
    "num_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import nn as dgl_nn\n",
    "from torch import nn\n",
    "\n",
    "class GAT_user(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(GAT_user, self).__init__()\n",
    "\n",
    "        self.embs = nn.Embedding.from_pretrained(learn_emb).requires_grad_(True)\n",
    "        self.layer1 = dgl_nn.GATv2Conv(embs_dim, hidden_dim, num_heads)\n",
    "        self.layer2 = dgl_nn.GATv2Conv(hidden_dim * num_heads, hidden_dim, num_heads)\n",
    "\n",
    "        self.do1 = nn.Dropout(0.4)\n",
    "        self.do2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.linear_weights = nn.Embedding.from_pretrained(linear_weights.float()).requires_grad_(True)  \n",
    "        \n",
    "        self.classify = nn.Linear(hidden_dim * num_heads, second_num_clusters)\n",
    "\n",
    "    def forward(self, bg):\n",
    "        x = bg.ndata['attr']\n",
    "        x_emb = bg.ndata['emb']\n",
    "        embeddings = self.embs.weight\n",
    "        all_embs = torch.concat((embeddings, centre_mass, intent_embs), dim = 1)\n",
    "        \n",
    "        get_embs = lambda i: all_embs[i]\n",
    "        node_embs = get_embs(x)\n",
    "        \n",
    "        result_embs = torch.concat((node_embs, x_emb), dim = 1)\n",
    "        h = result_embs.to(torch.float32)\n",
    "        \n",
    "        h = self.layer1(bg, h)\n",
    "        h = self.do1(h)\n",
    "        h = torch.reshape(h, (len(h), num_heads * hidden_dim))      \n",
    "        h = self.layer2(bg, h)\n",
    "        h = self.do2(h)\n",
    "\n",
    "        \n",
    "        bg.ndata['h'] = h\n",
    "        h = torch.reshape(h, (len(node_embs) // top_k, top_k, num_heads * hidden_dim))        \n",
    "        linear_weights_1dim = torch.reshape(self.linear_weights.weight, (top_k, ))\n",
    "        get_sum = lambda e: torch.matmul(linear_weights_1dim, e)\n",
    "        h = list(map(get_sum, h))\n",
    "        hg = torch.stack(h)\n",
    "        return self.classify(hg)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261932a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_model = GAT_user(hidden_dim, num_heads).to(device)\n",
    "user_train_epoch_losses = []\n",
    "user_valid_epoch_losses = []\n",
    "\n",
    "for param in user_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(user_model.parameters(), lr = 0.0001)\n",
    "lr_scheduler = LRScheduler(optimizer)\n",
    "early_stopping = EarlyStopping(10)\n",
    "\n",
    "user_num_epochs = 100\n",
    "\n",
    "for epoch in range(user_num_epochs):\n",
    "    train_epoch_loss = 0\n",
    "\n",
    "    for iter, (batched_graph, labels) in enumerate(user_train_data):\n",
    "        print(f\"{iter} / {len(user_train_data)}\")\n",
    "        out = user_model(batched_graph.to(device))\n",
    "        loss = criterion(out, labels.to(device))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        train_epoch_loss += loss.detach().item()\n",
    "        \n",
    "    train_epoch_loss /= (iter + 1)\n",
    "    user_train_epoch_losses.append(train_epoch_loss)\n",
    "    \n",
    "    valid_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, (batched_graph, labels) in enumerate(user_valid_data):\n",
    "            out = user_model(batched_graph.to(device))\n",
    "            loss = criterion(out, labels.to(device))\n",
    "            valid_epoch_loss += loss.detach().item()\n",
    "\n",
    "        valid_epoch_loss /= (iter + 1)\n",
    "        user_valid_epoch_losses.append(valid_epoch_loss)\n",
    "    print(f'Epoch {epoch}, train loss {train_epoch_loss:.4f}, valid loss {valid_epoch_loss:.4f}')  \n",
    "    wandb.log({'Epoch' : epoch, 'user train loss' : train_epoch_loss, 'user valid loss' : valid_epoch_loss})\n",
    "    \n",
    "    lr_scheduler(valid_epoch_loss)\n",
    "    early_stopping(valid_epoch_loss)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73876cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('user model cross entropy averaged over minibatches')\n",
    "plt.plot(user_train_epoch_losses, label = \"train\")\n",
    "plt.plot(user_valid_epoch_losses, label = \"valid\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34115eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model.eval()\n",
    "user_test_X, user_test_Y = map(list, zip(*user_test_data))\n",
    "\n",
    "user_probs = []\n",
    "user_test = []\n",
    "\n",
    "for i in range(len(user_test_Y)):\n",
    "    g = user_test_X[i].to(device)\n",
    "    labels = user_test_Y[i]\n",
    "    labels = labels.tolist()\n",
    "    user_test += labels\n",
    "    user_probs_Y = torch.softmax(user_model(g), 1).tolist()\n",
    "    user_probs += user_probs_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ee706",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USER metric\")\n",
    "\n",
    "print(\"Acc@1:\", get_accuracy_k(1, clusters.test_user_df, user_probs, clusters.test_dataset, 0))\n",
    "print(\"Acc@3:\", get_accuracy_k(3, clusters.test_user_df, user_probs, clusters.test_dataset, 0))\n",
    "print(\"Acc@5:\", get_accuracy_k(5, clusters.test_user_df, user_probs, clusters.test_dataset, 0))\n",
    "print(\"Acc@10:\", get_accuracy_k(10, clusters.test_user_df, user_probs, clusters.test_dataset, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ced9e",
   "metadata": {},
   "source": [
    "## 4.6 Prediction of system clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import nn as dgl_nn\n",
    "from torch import nn\n",
    "\n",
    "class GAT_system(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(GAT_system, self).__init__()\n",
    "\n",
    "        self.embs = nn.Embedding.from_pretrained(learn_emb).requires_grad_(True)\n",
    "        self.layer1 = dgl_nn.GATv2Conv(embs_dim, hidden_dim, num_heads)\n",
    "        self.layer2 = dgl_nn.GATv2Conv(hidden_dim * num_heads, hidden_dim, num_heads)\n",
    "\n",
    "        self.do1 = nn.Dropout(0.4)\n",
    "        self.do2 = nn.Dropout(0.4)\n",
    "\n",
    "        self.linear_weights = nn.Embedding.from_pretrained(linear_weights.float()).requires_grad_(True)  \n",
    "        \n",
    "        self.classify = nn.Linear(hidden_dim * num_heads, second_num_clusters)\n",
    "\n",
    "    def forward(self, bg):\n",
    "        x = bg.ndata['attr']\n",
    "        x_emb = bg.ndata['emb']\n",
    "        embeddings = self.embs.weight\n",
    "        all_embs = torch.concat((embeddings, centre_mass, intent_embs), dim = 1)\n",
    "        \n",
    "        get_embs = lambda i: all_embs[i]\n",
    "        node_embs = get_embs(x)\n",
    "        \n",
    "        result_embs = torch.concat((node_embs, x_emb), dim = 1)\n",
    "        h = result_embs.to(torch.float32)\n",
    "        \n",
    "        h = self.layer1(bg, h)\n",
    "        h = self.do1(h)\n",
    "        h = torch.reshape(h, (len(h), num_heads * hidden_dim))      \n",
    "        h = self.layer2(bg, h)\n",
    "        h = self.do2(h)\n",
    "\n",
    "        \n",
    "        bg.ndata['h'] = h\n",
    "        h = torch.reshape(h, (len(node_embs) // top_k, top_k, num_heads * hidden_dim))        \n",
    "        linear_weights_1dim = torch.reshape(self.linear_weights.weight, (top_k, ))\n",
    "        get_sum = lambda e: torch.matmul(linear_weights_1dim, e)\n",
    "        h = list(map(get_sum, h))\n",
    "        hg = torch.stack(h)\n",
    "        return self.classify(hg)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c1fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system_model = GAT_system(hidden_dim, num_heads).to(device)\n",
    "\n",
    "for param in system_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(system_model.parameters(), lr = 0.0001)\n",
    "lr_scheduler = LRScheduler(optimizer)\n",
    "early_stopping = EarlyStopping(10)\n",
    "\n",
    "sys_num_epochs = 100\n",
    "\n",
    "for epoch in range(sys_num_epochs):\n",
    "    train_epoch_loss = 0\n",
    "\n",
    "    for iter, (batched_graph, labels) in enumerate(sys_train_data):\n",
    "        print(f\"{iter}/{len(sys_train_data)}\")\n",
    "        out = system_model(batched_graph.to(device))\n",
    "        loss = criterion(out, labels.to(device))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        train_epoch_loss += loss.detach().item()\n",
    "        \n",
    "    train_epoch_loss /= (iter + 1)\n",
    "    \n",
    "    valid_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, (batched_graph, labels) in enumerate(sys_valid_data):\n",
    "            out = system_model(batched_graph.to(device))\n",
    "            loss = criterion(out, labels.to(device))\n",
    "            \n",
    "            valid_epoch_loss += loss.detach().item()\n",
    "\n",
    "        valid_epoch_loss /= (iter + 1)\n",
    "\n",
    "    print(f'Epoch {epoch}, train loss {train_epoch_loss:.4f}, valid loss {valid_epoch_loss:.4f}')  \n",
    "    wandb.log({'system train loss' : train_epoch_loss, 'system valid loss' : valid_epoch_loss})\n",
    "    lr_scheduler(valid_epoch_loss)\n",
    "    early_stopping(valid_epoch_loss)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02078fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_model.eval()\n",
    "system_test_X, system_test_Y = map(list, zip(*sys_test_data))\n",
    "\n",
    "system_probs = []\n",
    "system_test = []\n",
    "\n",
    "for i in range(len(system_test_Y)):\n",
    "    g = system_test_X[i].to(device)\n",
    "    labels = system_test_Y[i]\n",
    "    labels = labels.tolist()\n",
    "    system_test += labels\n",
    "    system_probs_Y = torch.softmax(system_model(g), 1).tolist()\n",
    "    system_probs += system_probs_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fae1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"SYSTEM metric\")\n",
    "\n",
    "print(\"Acc@1:\", get_accuracy_k(1, clusters.test_system_df, system_probs, clusters.test_dataset, 1))\n",
    "print(\"Acc@3:\", get_accuracy_k(3, clusters.test_system_df, system_probs, clusters.test_dataset, 1))\n",
    "print(\"Acc@5:\", get_accuracy_k(5, clusters.test_system_df, system_probs, clusters.test_dataset, 1))\n",
    "print(\"Acc@10:\", get_accuracy_k(10, clusters.test_system_df, system_probs, clusters.test_dataset, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ALL metric\")\n",
    "print(\"Acc@1:\", get_all_accuracy_k(1, clusters.test_user_df, clusters.test_system_df, user_probs, system_probs, clusters.test_dataset))\n",
    "print(\"Acc@3:\", get_all_accuracy_k(3, clusters.test_user_df, clusters.test_system_df, user_probs, system_probs, clusters.test_dataset))\n",
    "print(\"Acc@5:\", get_all_accuracy_k(5, clusters.test_user_df, clusters.test_system_df, user_probs, system_probs, clusters.test_dataset))\n",
    "print(\"Acc@10:\", get_all_accuracy_k(10, clusters.test_user_df, clusters.test_system_df, user_probs, system_probs, clusters.test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599042c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd805778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
